import streamlit as st
import pandas as pd
import numpy as np
import joblib
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®é¡µé¢
st.set_page_config(
    page_title="æ™ºèƒ½ä¿¡ç”¨è¯„çº§ç³»ç»Ÿ",
    page_icon="ğŸ¦",
    layout="wide"
)

# æ ‡é¢˜å’Œä»‹ç»
st.title("ğŸ¦ æ™ºèƒ½ä¿¡ç”¨è¯„çº§é¢„æµ‹ç³»ç»Ÿ")
st.markdown("""
åŸºäºé€»è¾‘å›å½’å’Œç¥ç»ç½‘ç»œçš„é›†æˆæ¨¡å‹ï¼Œç”¨äºé¢„æµ‹ä¸ªäºº/ä¼ä¸šçš„ä¿¡ç”¨ç­‰çº§ã€‚
""")

# åˆ›å»ºä¾§è¾¹æ 
st.sidebar.header("ğŸ”§ ç³»ç»Ÿé…ç½®")

# æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå‡½æ•°ï¼ˆå®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºçœŸå®æ•°æ®ï¼‰
def generate_sample_data(n_samples=20000):
    """ç”Ÿæˆæ¨¡æ‹Ÿçš„ä¿¡ç”¨æ•°æ®"""
    np.random.seed(42)
    
    data = {
        'age': np.random.randint(18, 70, n_samples),
        'income': np.random.normal(50000, 20000, n_samples),
        'credit_score': np.random.normal(650, 100, n_samples),
        'loan_amount': np.random.normal(20000, 10000, n_samples),
        'debt_to_income': np.random.normal(0.3, 0.15, n_samples),
        'employment_length': np.random.normal(5, 3, n_samples),
        'number_of_loans': np.random.randint(0, 10, n_samples),
        'delinquency_history': np.random.randint(0, 5, n_samples),
        'credit_history_length': np.random.normal(10, 5, n_samples)
    }
    
    df = pd.DataFrame(data)
    
    # ç”Ÿæˆç›®æ ‡å˜é‡ï¼ˆä¿¡ç”¨ç­‰çº§ 1-5ï¼‰
    # åŸºäºç‰¹å¾çš„çº¿æ€§ç»„åˆåŠ ä¸Šä¸€äº›å™ªå£°
    score = (
        0.1 * (df['income'] / 10000) +
        0.3 * (df['credit_score'] / 100) +
        -0.2 * df['debt_to_income'] * 10 +
        0.1 * (df['employment_length'] / 2) +
        -0.3 * df['delinquency_history'] +
        np.random.normal(0, 0.5, n_samples)
    )
    
    # å°†åˆ†æ•°æ˜ å°„åˆ°ä¿¡ç”¨ç­‰çº§ 1-5
    df['credit_grade'] = pd.cut(score, bins=5, labels=[1, 2, 3, 4, 5])
    
    return df

# æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
@st.cache_data
def load_and_preprocess_data():
    """åŠ è½½å’Œé¢„å¤„ç†æ•°æ®"""
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆå®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºä½ çš„æ•°æ®ï¼‰
    data = generate_sample_data(1000)  # ä½¿ç”¨è¾ƒå°æ•°æ®é›†ç”¨äºæ¼”ç¤º
    
    # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡
    X = data.drop('credit_grade', axis=1)
    y = data['credit_grade']
    
    # æ•°æ®æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    return X_scaled, y, X.columns, scaler

# æ¨¡å‹è®­ç»ƒ
@st.cache_resource
def train_models(X, y):
    """è®­ç»ƒé€»è¾‘å›å½’å’Œç¥ç»ç½‘ç»œæ¨¡å‹"""
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹
    st.sidebar.info("è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹ä¸­...")
    logreg_model = LogisticRegression(
        multi_class='multinomial',
        solver='lbfgs',
        max_iter=1000,
        random_state=42
    )
    logreg_model.fit(X_train, y_train)
    
    # è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹
    st.sidebar.info("è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹ä¸­...")
    nn_model = MLPClassifier(
        hidden_layer_sizes=(100, 50),
        activation='relu',
        solver='adam',
        max_iter=1000,
        random_state=42
    )
    nn_model.fit(X_train, y_train)
    
    return logreg_model, nn_model, X_test, y_test

# é›†æˆé¢„æµ‹
def ensemble_predict(logreg_model, nn_model, X, method='average'):
    """é›†æˆæ¨¡å‹é¢„æµ‹"""
    if method == 'average':
        # å¹³å‡æ¦‚ç‡
        logreg_proba = logreg_model.predict_proba(X)
        nn_proba = nn_model.predict_proba(X)
        avg_proba = (logreg_proba + nn_proba) / 2
        return np.argmax(avg_proba, axis=1) + 1  # +1 å› ä¸ºä¿¡ç”¨ç­‰çº§ä»1å¼€å§‹
    else:
        # æŠ•ç¥¨æœºåˆ¶
        logreg_pred = logreg_model.predict(X)
        nn_pred = nn_model.predict(X)
        # ç®€å•æŠ•ç¥¨ï¼ˆå¯ä»¥æ”¹è¿›ä¸ºåŠ æƒæŠ•ç¥¨ï¼‰
        return np.round((logreg_pred.astype(int) + nn_pred.astype(int)) / 2).astype(int)

# ä¸»åº”ç”¨
def main():
    # åŠ è½½æ•°æ®
    X, y, feature_names, scaler = load_and_preprocess_data()
    
    # è®­ç»ƒæ¨¡å‹
    logreg_model, nn_model, X_test, y_test = train_models(X, y)
    
    # ä¾§è¾¹æ  - æ¨¡å‹é€‰æ‹©
    model_choice = st.sidebar.selectbox(
        "é€‰æ‹©é¢„æµ‹æ¨¡å‹:",
        ["é›†æˆæ¨¡å‹", "é€»è¾‘å›å½’", "ç¥ç»ç½‘ç»œ"]
    )
    
    # ä¾§è¾¹æ  - ç‰¹å¾é‡è¦æ€§
    if st.sidebar.checkbox("æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§"):
        show_feature_importance(logreg_model, feature_names)
    
    # ä¸»ç•Œé¢æ ‡ç­¾é¡µ
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ“Š å•æ ·æœ¬é¢„æµ‹", 
        "ğŸ“ æ‰¹é‡é¢„æµ‹", 
        "ğŸ“ˆ æ¨¡å‹è¯„ä¼°", 
        "â„¹ï¸ ç³»ç»Ÿä¿¡æ¯"
    ])
    
    with tab1:
        single_prediction_interface(logreg_model, nn_model, scaler, feature_names, model_choice)
    
    with tab2:
        batch_prediction_interface(logreg_model, nn_model, scaler, model_choice)
    
    with tab3:
        model_evaluation_interface(logreg_model, nn_model, X_test, y_test, model_choice)
    
    with tab4:
        system_info_interface()

def single_prediction_interface(logreg_model, nn_model, scaler, feature_names, model_choice):
    """å•æ ·æœ¬é¢„æµ‹ç•Œé¢"""
    st.header("ğŸ” å•æ ·æœ¬ä¿¡ç”¨è¯„çº§é¢„æµ‹")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("è¾“å…¥ä¸ªäººä¿¡æ¯")
        
        # åˆ›å»ºè¾“å…¥è¡¨å•
        with st.form("prediction_form"):
            age = st.slider("å¹´é¾„", 18, 70, 35)
            income = st.number_input("å¹´æ”¶å…¥ ($)", 10000, 200000, 50000, step=1000)
            credit_score = st.slider("ä¿¡ç”¨åˆ†æ•°", 300, 850, 650)
            loan_amount = st.number_input("è´·æ¬¾é‡‘é¢ ($)", 1000, 100000, 20000, step=1000)
            debt_to_income = st.slider("å€ºåŠ¡æ”¶å…¥æ¯”", 0.1, 1.0, 0.3, 0.05)
            employment_length = st.slider("å·¥ä½œå¹´é™", 0, 30, 5)
            number_of_loans = st.slider("ç°æœ‰è´·æ¬¾æ•°é‡", 0, 10, 2)
            delinquency_history = st.slider("é€¾æœŸè®°å½•æ¬¡æ•°", 0, 10, 0)
            credit_history_length = st.slider("ä¿¡ç”¨å†å²é•¿åº¦ (å¹´)", 0, 30, 10)
            
            submitted = st.form_submit_button("é¢„æµ‹ä¿¡ç”¨ç­‰çº§")
    
    with col2:
        st.subheader("é¢„æµ‹ç»“æœ")
        
        if submitted:
            # å‡†å¤‡è¾“å…¥æ•°æ®
            input_data = np.array([[
                age, income, credit_score, loan_amount, debt_to_income,
                employment_length, number_of_loans, delinquency_history,
                credit_history_length
            ]])
            
            # æ ‡å‡†åŒ–
            input_scaled = scaler.transform(input_data)
            
            # æ ¹æ®é€‰æ‹©çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
            if model_choice == "é€»è¾‘å›å½’":
                prediction = logreg_model.predict(input_scaled)[0]
                probability = logreg_model.predict_proba(input_scaled)[0]
            elif model_choice == "ç¥ç»ç½‘ç»œ":
                prediction = nn_model.predict(input_scaled)[0]
                probability = nn_model.predict_proba(input_scaled)[0]
            else:  # é›†æˆæ¨¡å‹
                prediction = ensemble_predict(logreg_model, nn_model, input_scaled)[0]
                # å¯¹äºé›†æˆæ¨¡å‹ï¼Œæˆ‘ä»¬è®¡ç®—å¹³å‡æ¦‚ç‡
                logreg_proba = logreg_model.predict_proba(input_scaled)[0]
                nn_proba = nn_model.predict_proba(input_scaled)[0]
                probability = (logreg_proba + nn_proba) / 2
            
            # æ˜¾ç¤ºç»“æœ
            display_prediction_result(prediction, probability)
            
            # æ˜¾ç¤ºç‰¹å¾å½±å“åˆ†æ
            st.subheader("ğŸ“‹ ç‰¹å¾å½±å“åˆ†æ")
            feature_analysis = analyze_feature_impact(
                logreg_model, input_scaled[0], feature_names
            )
            st.dataframe(feature_analysis)

def display_prediction_result(prediction, probability):
    """æ˜¾ç¤ºé¢„æµ‹ç»“æœ"""
    # ä¿¡ç”¨ç­‰çº§æè¿°
    grade_descriptions = {
        1: "ä¼˜ç§€ - é£é™©æä½",
        2: "è‰¯å¥½ - é£é™©è¾ƒä½", 
        3: "ä¸€èˆ¬ - ä¸­ç­‰é£é™©",
        4: "å…³æ³¨ - é£é™©è¾ƒé«˜",
        5: "ä¸è‰¯ - é«˜é£é™©"
    }
    
    # é¢œè‰²æ˜ å°„
    grade_colors = {
        1: "ğŸŸ¢", 2: "ğŸŸ¡", 3: "ğŸŸ ", 4: "ğŸ”´", 5: "âš«"
    }
    
    # æ˜¾ç¤ºä¸»è¦ç»“æœ
    st.metric(
        label="é¢„æµ‹ä¿¡ç”¨ç­‰çº§",
        value=f"{grade_colors[prediction]} ç­‰çº§ {prediction}",
        delta=grade_descriptions[prediction]
    )
    
    # æ˜¾ç¤ºæ¦‚ç‡åˆ†å¸ƒ
    st.subheader("å„ç­‰çº§é¢„æµ‹æ¦‚ç‡")
    
    prob_df = pd.DataFrame({
        'ä¿¡ç”¨ç­‰çº§': [f'ç­‰çº§ {i}' for i in range(1, 6)],
        'æ¦‚ç‡': probability,
        'æè¿°': [grade_descriptions[i] for i in range(1, 6)]
    })
    
    # æ˜¾ç¤ºæ¦‚ç‡æ¡å½¢å›¾
    fig, ax = plt.subplots(figsize=(10, 6))
    colors = ['green', 'lightgreen', 'yellow', 'orange', 'red']
    bars = ax.bar(prob_df['ä¿¡ç”¨ç­‰çº§'], prob_df['æ¦‚ç‡'] * 100, color=colors)
    
    # åœ¨æ¡å½¢ä¸Šæ·»åŠ æ•°å€¼
    for bar, prob in zip(bars, prob_df['æ¦‚ç‡']):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 1,
                f'{prob*100:.1f}%', ha='center', va='bottom')
    
    ax.set_ylabel('æ¦‚ç‡ (%)')
    ax.set_ylim(0, 100)
    ax.set_title('å„ä¿¡ç”¨ç­‰çº§é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒ')
    st.pyplot(fig)

def analyze_feature_impact(model, input_features, feature_names):
    """åˆ†æç‰¹å¾å¯¹é¢„æµ‹çš„å½±å“"""
    if hasattr(model, 'coef_'):
        # å¯¹äºé€»è¾‘å›å½’æ¨¡å‹
        coefficients = model.coef_[0]  # å–ç¬¬ä¸€ä¸ªç±»çš„ç³»æ•°ï¼ˆå¤šåˆ†ç±»æ—¶å¯èƒ½éœ€è¦è°ƒæ•´ï¼‰
        impact = coefficients * input_features
        
        impact_df = pd.DataFrame({
            'ç‰¹å¾': feature_names,
            'åŸå§‹å€¼': input_features,
            'ç³»æ•°æƒé‡': coefficients,
            'å½±å“ç¨‹åº¦': impact,
            'å½±å“æ–¹å‘': ['æ­£å‘' if x > 0 else 'è´Ÿå‘' for x in impact]
        })
        
        return impact_df.sort_values('å½±å“ç¨‹åº¦', key=abs, ascending=False)
    else:
        return pd.DataFrame({'æ¶ˆæ¯': ['ç‰¹å¾å½±å“åˆ†æä»…é€‚ç”¨äºé€»è¾‘å›å½’æ¨¡å‹']})

def batch_prediction_interface(logreg_model, nn_model, scaler, model_choice):
    """æ‰¹é‡é¢„æµ‹ç•Œé¢"""
    st.header("ğŸ“ æ‰¹é‡ä¿¡ç”¨è¯„çº§é¢„æµ‹")
    
    uploaded_file = st.file_uploader(
        "ä¸Šä¼ CSVæ–‡ä»¶è¿›è¡Œæ‰¹é‡é¢„æµ‹", 
        type=['csv'],
        help="è¯·ç¡®ä¿æ–‡ä»¶åŒ…å«æ‰€éœ€çš„ç‰¹å¾åˆ—"
    )
    
    if uploaded_file is not None:
        try:
            # è¯»å–æ•°æ®
            batch_data = pd.read_csv(uploaded_file)
            st.success(f"æˆåŠŸè¯»å–æ•°æ®ï¼Œå…± {len(batch_data)} æ¡è®°å½•")
            
            # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
            st.subheader("æ•°æ®é¢„è§ˆ")
            st.dataframe(batch_data.head())
            
            if st.button("å¼€å§‹æ‰¹é‡é¢„æµ‹"):
                # è¿™é‡Œåº”è¯¥æ·»åŠ æ•°æ®éªŒè¯å’Œé¢„å¤„ç†
                # å‡è®¾æ•°æ®å·²ç»åŒ…å«æ‰€éœ€ç‰¹å¾
                
                # æ ‡å‡†åŒ–æ•°æ®
                batch_scaled = scaler.transform(batch_data)
                
                # æ‰¹é‡é¢„æµ‹
                if model_choice == "é€»è¾‘å›å½’":
                    predictions = logreg_model.predict(batch_scaled)
                elif model_choice == "ç¥ç»ç½‘ç»œ":
                    predictions = nn_model.predict(batch_scaled)
                else:  # é›†æˆæ¨¡å‹
                    predictions = ensemble_predict(logreg_model, nn_model, batch_scaled)
                
                # æ·»åŠ é¢„æµ‹ç»“æœåˆ°æ•°æ®
                batch_data['é¢„æµ‹ä¿¡ç”¨ç­‰çº§'] = predictions
                
                # æ˜¾ç¤ºç»“æœ
                st.subheader("æ‰¹é‡é¢„æµ‹ç»“æœ")
                st.dataframe(batch_data)
                
                # ç»Ÿè®¡ç»“æœ
                st.subheader("é¢„æµ‹ç»“æœç»Ÿè®¡")
                result_counts = batch_data['é¢„æµ‹ä¿¡ç”¨ç­‰çº§'].value_counts().sort_index()
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.dataframe(result_counts)
                
                with col2:
                    fig, ax = plt.subplots()
                    result_counts.plot(kind='bar', ax=ax, color=['green', 'lightgreen', 'yellow', 'orange', 'red'])
                    ax.set_title('ä¿¡ç”¨ç­‰çº§åˆ†å¸ƒ')
                    ax.set_xlabel('ä¿¡ç”¨ç­‰çº§')
                    ax.set_ylabel('æ•°é‡')
                    st.pyplot(fig)
                
                # ä¸‹è½½ç»“æœ
                csv = batch_data.to_csv(index=False)
                st.download_button(
                    label="ä¸‹è½½é¢„æµ‹ç»“æœ",
                    data=csv,
                    file_name="ä¿¡ç”¨è¯„çº§é¢„æµ‹ç»“æœ.csv",
                    mime="text/csv"
                )
                
        except Exception as e:
            st.error(f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")

def model_evaluation_interface(logreg_model, nn_model, X_test, y_test, model_choice):
    """æ¨¡å‹è¯„ä¼°ç•Œé¢"""
    st.header("ğŸ“ˆ æ¨¡å‹æ€§èƒ½è¯„ä¼°")
    
    # æ ¹æ®é€‰æ‹©çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹
    if model_choice == "é€»è¾‘å›å½’":
        y_pred = logreg_model.predict(X_test)
        model_name = "é€»è¾‘å›å½’"
    elif model_choice == "ç¥ç»ç½‘ç»œ":
        y_pred = nn_model.predict(X_test)
        model_name = "ç¥ç»ç½‘ç»œ"
    else:  # é›†æˆæ¨¡å‹
        y_pred = ensemble_predict(logreg_model, nn_model, X_test)
        model_name = "é›†æˆæ¨¡å‹"
    
    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(y_test, y_pred)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("å‡†ç¡®ç‡", f"{accuracy:.3f}")
    
    with col2:
        st.metric("æµ‹è¯•æ ·æœ¬æ•°", len(y_test))
    
    with col3:
        st.metric("æ¨¡å‹", model_name)
    
    # æ··æ·†çŸ©é˜µ
    st.subheader("æ··æ·†çŸ©é˜µ")
    cm = confusion_matrix(y_test, y_pred)
    
    fig, ax = plt.subplots(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)
    ax.set_xlabel('é¢„æµ‹æ ‡ç­¾')
    ax.set_ylabel('çœŸå®æ ‡ç­¾')
    ax.set_title(f'{model_name} - æ··æ·†çŸ©é˜µ')
    st.pyplot(fig)
    
    # åˆ†ç±»æŠ¥å‘Š
    st.subheader("è¯¦ç»†åˆ†ç±»æŠ¥å‘Š")
    report = classification_report(y_test, y_pred, output_dict=True)
    report_df = pd.DataFrame(report).transpose()
    st.dataframe(report_df.style.format("{:.3f}"))

def show_feature_importance(model, feature_names):
    """æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§"""
    st.sidebar.subheader("ç‰¹å¾é‡è¦æ€§")
    
    if hasattr(model, 'coef_'):
        # é€»è¾‘å›å½’çš„ç‰¹å¾é‡è¦æ€§
        importance = np.abs(model.coef_[0])
        feature_imp = pd.DataFrame({
            'ç‰¹å¾': feature_names,
            'é‡è¦æ€§': importance
        }).sort_values('é‡è¦æ€§', ascending=False)
        
        for _, row in feature_imp.head().iterrows():
            st.sidebar.write(f"â€¢ {row['ç‰¹å¾']}: {row['é‡è¦æ€§']:.3f}")
    else:
        st.sidebar.info("ç¥ç»ç½‘ç»œæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§åˆ†æè¾ƒå¤æ‚")

def system_info_interface():
    """ç³»ç»Ÿä¿¡æ¯ç•Œé¢"""
    st.header("â„¹ï¸ ç³»ç»Ÿä¿¡æ¯")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ç³»ç»Ÿæ¶æ„")
        st.markdown("""
        - **å‰ç«¯**: Streamlit
        - **æœºå™¨å­¦ä¹ æ¡†æ¶**: Scikit-learn
        - **ä¸»è¦ç®—æ³•**: 
          - é€»è¾‘å›å½’ (å¤šåˆ†ç±»)
          - ç¥ç»ç½‘ç»œ (MLP)
          - é›†æˆå­¦ä¹ 
        - **æ•°æ®å¤„ç†**: Pandas, NumPy
        """)
    
    with col2:
        st.subheader("æ¨¡å‹ç‰¹æ€§")
        st.markdown("""
        - âœ… æ”¯æŒå•æ ·æœ¬å®æ—¶é¢„æµ‹
        - âœ… æ”¯æŒæ‰¹é‡æ•°æ®å¤„ç†
        - âœ… æ¨¡å‹æ€§èƒ½å¯è§†åŒ–
        - âœ… ç‰¹å¾å½±å“åˆ†æ
        - âœ… ç»“æœå¯¼å‡ºåŠŸèƒ½
        """)
    
    st.subheader("ä½¿ç”¨è¯´æ˜")
    st.markdown("""
    1. **å•æ ·æœ¬é¢„æµ‹**: åœ¨"å•æ ·æœ¬é¢„æµ‹"æ ‡ç­¾é¡µä¸­è¾“å…¥ä¸ªäººä¿¡ç”¨ä¿¡æ¯
    2. **æ‰¹é‡é¢„æµ‹**: åœ¨"æ‰¹é‡é¢„æµ‹"æ ‡ç­¾é¡µä¸­ä¸Šä¼ CSVæ–‡ä»¶
    3. **æ¨¡å‹è¯„ä¼°**: æŸ¥çœ‹ä¸åŒæ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡
    4. **ç»“æœè§£è¯»**: ä¿¡ç”¨ç­‰çº§1-5ï¼Œæ•°å­—è¶Šå°ä¿¡ç”¨è¶Šå¥½
    """)

# è¿è¡Œåº”ç”¨
if __name__ == "__main__":
    main()
